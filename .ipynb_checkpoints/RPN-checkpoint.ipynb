{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Faster R-CNN - Region Proposal Network (RPN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faster R-CNN consists of two primary networks. \n",
    "\n",
    "**1) Region Proposal Network**\n",
    "\n",
    "**2) Fast R-CNN**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Architecture](arch.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source : https://towardsdatascience.com/faster-rcnn-object-detection-f865e5ed7fc4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Concept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faster R-CNN is an anchor-based model. This means that anchors (bounding boxes) are placed consistently throughout a whole image. There are different sized anchors. The number of anchors and size of anchors are hyperparameters. These anchors will be used as references to ground-truth boxes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input to the model is a fixed size image. The RPN has 2 different outputs. The first output is whether an anchor contains an object or not. (Anchor contains object when IoU > 0.7 (This threshold can be changed)). The second output is the difference between the coordinates of the ground-truth box and the anchor box. The difference is calculated as per the formula given in the original research paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anchors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RPN consists of convolutional filters throughout the entire network. When an image is given as the input to the network, the subsampled feature map from the last convolutional layer will be used as reference to place the anchors. For example, let's assume that all the convolutional layers used zero-padding thus preserving the size of the input while there were 3 max pooling layers in between such that each pooling process reduces the size by half."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that our input image is subsampled to 2^3=8 times its original size. For the sake of example, let's say the input image was 224 x 224 x 3 in size. Therefore, the last feature map would be of size 28 x 28 x 256 (assuming the final convolution process outputs 256 filters). On this feature map is where our first RPN process takes place. The first process is to convolve this feature map with an N x N filter (More explanation on this later)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The anchors will be placed based on this kernel size. Let's say the kernel size is 3 x 3. The kernel will have a stride of 1 with no padding. The center of this kernel is where the anchors' center will be. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Anchor_Center](anchor_center.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source : https://www.researchgate.net/figure/Anchor-boxes-of-the-Faster-R-CNN-R-CNN-region-proposal-convolution-neural-networks_fig1_324060834"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the figure above, 9 anchors of different sizes were placed on each of the centers. Each anchor is denoted by [x,y,w,h]. \n",
    "\n",
    "x = x-coor of the anchor's center\n",
    "\n",
    "y = y-coor of the anchor's center\n",
    "\n",
    "w = width of the anchor\n",
    "\n",
    "h = height of the anchor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE** that the coordinate of these anchors are based on the original image. In our case, the 28 x 28 feature map has a kernel of 3 x 3. The real input was subsampled to 8th of its original size. Therefore, each of the anchor's center, (1,1), (2,1), (3,1)... on the feature map will be multiplied by 8 to obtain the anchor's center on the original input image. Since there is no padding used, and there are 9 anchors in each location, we have 26 x 26 x 9 = 6084 anchors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The anchors are placed consistently for all the training images before the training of the model begins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Region Proposal Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the 28 x 28 x 256 feature maps are presented, the first process of RPN would be another convolution process using a 3 x 3 filter. The resultant 26 x 26 x 256 filters will now be an input to two separate convolution process that uses a filter of 1 x 1. One of these two process is responsible to provide a confidence score whether an object is present or not in the anchor box. The other provides the [x,y,w,h] values for the anchor box to make adjustments to fit the detected object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RPN](rpn.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source : https://arxiv.org/pdf/1506.01497.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values for the regression is obtained by following the formula stated in the original research paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The convolution process that obtains the confidence scores has an output size of 9 x 2 = 18 (One hot label for 2 classes - whether an object is present or not) on each slide while the other process has an output size of 9 x 4 = 36 (each anchor has 4 coordinate) on each slide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training of RPN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are few things that should be done before the training of the RPN.\n",
    "\n",
    "1) Anchors that are out of image boundary should be ignored.\n",
    "\n",
    "2) IoU has to be calculated for each anchor and each ground-truth box.\n",
    "\n",
    "3) Anchors with more than a set threshold (example 0.7) of IoU will be labelled as positive anchors while anchors with less than a set threshold (example 0.3) of IoU will be labelled as negative anchors. \n",
    "\n",
    "4) Anchors that are between the two set IoUs will be ignored.\n",
    "\n",
    "5) During training, a small minibatch of anchors will be randomly sampled with equal number of negative and positive anchors.\n",
    "\n",
    "6) Only the positive anchors will go through the regression training. Other anchors will be ignored.\n",
    "\n",
    "7) All ignored anchors should not contribute to the loss."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nasa",
   "language": "python",
   "name": "nasa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
