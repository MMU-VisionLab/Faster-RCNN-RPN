{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Region Proposal Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Program Flow**\n",
    "\n",
    "1) Preprocess data to fit the format of Faster R-CNN.\n",
    "   - Extract information from the image and label files of Pascal VOC.\n",
    "   - Generate anchors based on a fixed kernel size.\n",
    "   - Keep track of list of anchors that should be ignored (out of boundary anchors).\n",
    "   - Determine positive and negative anchors for each image data (based on the ground-truth boxes).\n",
    "   - Determine the difference between positive anchors and the corresponding ground-truth objects based on the given formula (how much the anchor needs be adjusted to fit the ground-truth box).\n",
    "   - Sample a fixed amount of anchors for training (by using the list to keep track of ignored anchors earlier).\n",
    "\n",
    "2) Build RPN model.\n",
    "    - Make sure the kernel size selected is the same with the kernel size used when generating anchors earlier.\n",
    "    - Make sure the ignored anchors do not contribute to loss.\n",
    "    \n",
    "3) Post-Processing.\n",
    "    - Choose prediction anchors that are above a certain confidence level to represent the bounding boxes.\n",
    "    - Use the correct anchors (generated ones) to apply the regression results to. \n",
    "    - Use Non-Max Suppression technique to filter out multiple bounding box predictions on one object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import xmltodict\n",
    "import tensorflow as tf\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_images_path     = '../../VOCdevkit/VOC2012/JPEGImages'\n",
    "data_annotation_path = '../../VOCdevkit/VOC2012/Annotations'\n",
    "image_height = 224\n",
    "image_width  = 224\n",
    "image_depth  = 3\n",
    "rpn_kernel_size = 3\n",
    "subsampled_ratio = 8\n",
    "anchor_sizes = [32,64,128]\n",
    "anchor_aspect_ratio = [[1,1],[1,2],[2,1]]\n",
    "num_anchors_in_box = len(anchor_sizes)*len(anchor_aspect_ratio)\n",
    "neg_threshold = 0.3\n",
    "pos_threshold = 0.7\n",
    "anchor_sampling_amount = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the image and annotation file paths\n",
    "list_images      = sorted([x for x in glob.glob(data_images_path + '/**')])     #length : 17125\n",
    "list_annotations = sorted([x for x in glob.glob(data_annotation_path + '/**')]) #length : 17125\n",
    "total_images = len(list_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classes(xml_files=list_annotations):\n",
    "    '''\n",
    "    Output: All the distinct classes for this dataset.\n",
    "    \n",
    "    '''\n",
    "    classes = []\n",
    "    \n",
    "    for file in xml_files: \n",
    "\n",
    "        f = open(file)\n",
    "        doc = xmltodict.parse(f.read()) #parse the xml file to python dict.\n",
    "\n",
    "        #Images in the dataset might contain either 1 object or more than 1 object. For images with 1 object, the annotation for the object\n",
    "        #in the xml file will be located in 'annotation' -> 'object' -> 'name'. For images with more than 1 object, the annotations for the objects\n",
    "        #will be nested in 'annotation' -> 'object' thus requiring a loop to iterate through them. (Pascal VOC format)\n",
    "\n",
    "        try: \n",
    "            #try iterating through the tag. (For images with more than 1 obj.)\n",
    "            for obj in doc['annotation']['object']:\n",
    "                classes.append(obj['name'].lower()) #append the lowercased string.\n",
    "\n",
    "        except TypeError as e: #iterating through non-nested tags would throw a TypeError.\n",
    "            classes.append(doc['annotation']['object']['name'].lower()) #append the lowercased string.\n",
    "\n",
    "        f.close()\n",
    "\n",
    "    classes = list(set(classes)) #remove duplicates.\n",
    "    classes.sort()\n",
    "\n",
    "    #returns a list containing the names of classes after being sorted.\n",
    "    return classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = get_classes()\n",
    "num_of_class = len(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels_from_xml(xml_file_path, num_of_class = num_of_class):\n",
    "    '''\n",
    "    Input : A SINGLE xml file and the total number of classes in the dataset. \n",
    "    Output: Labels in numpy array format (Object classes their corresponding bounding box coordinates).\n",
    "\n",
    "    Desc : This function parses a single xml file and outputs the objects classes and their corresponding bounding box coordinates\n",
    "           [top-left-x, top-left-y, btm-right-x, btm-right-y] on the resized image.\n",
    "\n",
    "    '''\n",
    "\n",
    "    f = open(xml_file_path)\n",
    "    doc = xmltodict.parse(f.read()) #parse the xml file to python dict.\n",
    "\n",
    "    #get the original image height and width. Images have different height and width from each other.\n",
    "    ori_img_height = float(doc['annotation']['size']['height'])\n",
    "    ori_img_width  = float(doc['annotation']['size']['width'])\n",
    "\n",
    "\n",
    "    class_label = [] #init for keeping track objects' labels.\n",
    "    bbox_label  = [] #init for keeping track of objects' bounding box (bb).\n",
    "\n",
    "\n",
    "    #Images in the dataset might contain either 1 object or more than 1 object. For images with 1 object, the annotation for the object\n",
    "    #in the xml file will be located in 'annotation' -> 'object' -> 'name'. For images with more than 1 object, the annotations for the objects\n",
    "    #will be nested in 'annotation' -> 'object' thus requiring a loop to iterate through them. (Pascal VOC format)\n",
    "    try:\n",
    "        #Try iterating through the tag (For images with more than 1 obj).\n",
    "        for each_obj in doc['annotation']['object']:\n",
    "\n",
    "            obj_class = each_obj['name'].lower() #get the label for the object and lowercase the string.\n",
    "\n",
    "            #Pascal VOC's format to denote bounding boxes are to denote the top left part of the box and the bottom right of the box.\n",
    "            #the coordinates are in terms of x and y axis for both part of the box.\n",
    "            x_min = float(each_obj['bndbox']['xmin']) #top left x-axis coordinate.\n",
    "            x_max = float(each_obj['bndbox']['xmax']) #bottom right x-axis coordinate.\n",
    "            y_min = float(each_obj['bndbox']['ymin']) #top left y-axis coordinate.\n",
    "            y_max = float(each_obj['bndbox']['ymax']) #bottom right y-axis coordinate.\n",
    "\n",
    "        ##################################################################################\n",
    "        #We want to make sure the coordinates are resized according to the resized image.#\n",
    "        ##################################################################################\n",
    "\n",
    "            #All the images will be resized to a fixed size in order to be fixed-size inputs to the neural network model.\n",
    "            #Therefore, we need to resize the coordinates as well since the coordinates above is based on the original size of the images.\n",
    "\n",
    "            #In order to find the resized coordinates, we must multiply the ratio of the resized image compared to its original to the coordinates.\n",
    "            x_min = float((image_width/ori_img_width)*x_min)\n",
    "            y_min = float((image_height/ori_img_height)*y_min)\n",
    "            x_max = float((image_width/ori_img_width)*x_max)\n",
    "            y_max = float((image_height/ori_img_height)*y_max)\n",
    "\n",
    "            generated_box_info = [x_min, y_min, x_max, y_max]\n",
    "\n",
    "\n",
    "            index = classes.index(obj_class) #get the index of the object's class.\n",
    "\n",
    "            #append each object's class label and the bounding box label (converted to Faster R-CNN format) into the list initialized earlier.\n",
    "            class_label.append(index)\n",
    "            bbox_label.append(np.asarray(generated_box_info, dtype='float32'))\n",
    "\n",
    "    except TypeError as e : #happens when the iteration through the tag fails due to only 1 object being in the image.\n",
    "\n",
    "        #SAME PROCEDURE AS ABOVE !  \n",
    "\n",
    "        #Getting these information from the XML file differs compared to above,\n",
    "        obj_class = doc['annotation']['object']['name']\n",
    "        x_min = float(doc['annotation']['object']['bndbox']['xmin']) \n",
    "        x_max = float(doc['annotation']['object']['bndbox']['xmax']) \n",
    "        y_min = float(doc['annotation']['object']['bndbox']['ymin']) \n",
    "        y_max = float(doc['annotation']['object']['bndbox']['ymax']) \n",
    "\n",
    "        x_min = float((image_width/ori_img_width)*x_min)\n",
    "        y_min = float((image_height/ori_img_height)*y_min)\n",
    "        x_max = float((image_width/ori_img_width)*x_max)\n",
    "        y_max = float((image_height/ori_img_height)*y_max)\n",
    "\n",
    "        generated_box_info = [x_min, y_min, x_max, y_max]\n",
    "\n",
    "        #Get the index of the class\n",
    "        index = classes.index(obj_class) \n",
    "\n",
    "        class_label.append(index)\n",
    "        bbox_label.append(np.asarray(generated_box_info, dtype='float32'))\n",
    "\n",
    "\n",
    "    return class_label, np.asarray(bbox_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_anchors(rpn_kernel_size=rpn_kernel_size, subsampled_ratio=subsampled_ratio,\n",
    "                     anchor_sizes=anchor_sizes, anchor_aspect_ratio=anchor_aspect_ratio):\n",
    "\n",
    "    '''\n",
    "    Since Faster R-CNN works by placing different sized anchors throughout an image on different positions, we need to generate the coordinates\n",
    "    for each of those anchors first before generating the labels required by Faster R-CNN. This function generates the anchors based on number\n",
    "    of anchors, sizes of anchors, aspect ratios of anchors, RPN's kernel size and subsampled size of the image.\n",
    "\n",
    "    Output : list of anchors (each anchor is denoted by (x,y,w,h)) and the list if an anchor should be ignored or not.\n",
    "\n",
    "    '''\n",
    "\n",
    "    list_of_anchors = []\n",
    "    anchor_booleans = [] #This is to keep track of an anchor's status. Anchors that are out of boundary are meant to be ignored.\n",
    "\n",
    "    #get the feature map's height and width after convolutions and poolings.\n",
    "\n",
    "\n",
    "    #the anchor's center is always at the middle grid of the RPN kernel. Therefore, we need to imitate the kernel slide to get all the\n",
    "    #centers of the anchor. If a kernel is 3x3, the center will start from (1,1). If a kernel is 5x5, the center will start from (2,2).\n",
    "    #we use divmod to get the first coordinate of the RPN kernel's middle point.\n",
    "    starting_center = divmod(rpn_kernel_size, 2)[0]\n",
    "\n",
    "    anchor_center = [starting_center - 1,starting_center] #-1 on the x-coor because the increment comes first in the while loop below.\n",
    "\n",
    "    #We want to imitate the kernel sliding with stride 1 until it reaches the ending center. Since index starts with 0, we subtract 1 more to the \n",
    "    #width and height of the subsampled image.\n",
    "    subsampled_height = image_height/subsampled_ratio\n",
    "    subsampled_width = image_width/subsampled_ratio\n",
    "    \n",
    "    while (anchor_center != [subsampled_width - (1 + starting_center), subsampled_height - (1 + starting_center)]): \n",
    "\n",
    "\n",
    "        anchor_center[0] += 1 #Increment the x-axis by 1.\n",
    "\n",
    "        #If the sliding window has reached the last central point at the right side, increase the y-axis by 1 and \n",
    "        #reset x-axis to 0.\n",
    "        if anchor_center[0] > subsampled_width - (1 + starting_center):\n",
    "\n",
    "            anchor_center[1] += 1\n",
    "            anchor_center[0] = starting_center\n",
    "\n",
    "        #Even though we calculate the anchor on the feature map, the anchors are still referenced to the original image. Therefore, \n",
    "        #once we obtain the position of the center of the anchor on the feature map, we multiply is by the downsampling ratio to obtain its \n",
    "        #center position referenced to the original input image.\n",
    "        anchor_center_on_image = [anchor_center[0]*subsampled_ratio, anchor_center[1]*subsampled_ratio]\n",
    "\n",
    "        #We want to calculate the anchor's height and width on all the different variations of aspect ratio and sizes.\n",
    "        #Iterate through every size defined for each anchor center's position.\n",
    "        for size in anchor_sizes:\n",
    "\n",
    "            #Iterate through every aspect ratio for each size.\n",
    "            for a_ratio in anchor_aspect_ratio:\n",
    "\n",
    "                #[x,y,w,h] of an anchor.\n",
    "                anchor_info = [anchor_center_on_image[0], anchor_center_on_image[1], size*a_ratio[0], size*a_ratio[1]]\n",
    "\n",
    "                #Perform check if a given anchor crosses the boundary of the image or not. Such anchors are to be ignored and will be labelled as 0.\n",
    "                #Else the anchor will be labelled as 1 (meaning good to go).\n",
    "                if (anchor_info[0] - anchor_info[2]/2 < 0 or anchor_info[0] + anchor_info[2]/2 > image_width or \n",
    "                                        anchor_info[1] - anchor_info[3]/2 < 0 or anchor_info[1] + anchor_info[3]/2 > image_height) :\n",
    "\n",
    "                    anchor_booleans.append([0.0])\n",
    "\n",
    "                else:\n",
    "\n",
    "                    anchor_booleans.append([1.0])\n",
    "\n",
    "                list_of_anchors.append(anchor_info)\n",
    "\n",
    "    return list_of_anchors, anchor_booleans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IoU_calculator(box1, box2, area1=None, area2=None):\n",
    "    '''\n",
    "    Inputs two boxes (each with top-left-x, top-left-y, btm-right-x, btm-right-y coordinates).\n",
    "    If the area of one or two of the boxes are known, it can be passed here as well.\n",
    "    Calculates the IoU between the two boxes and returns the IoU.\n",
    "    '''\n",
    "    \n",
    "    #Get the area of the boxes\n",
    "    if area1 is None:\n",
    "        #Area of the box1. +1 since index starts from 0.\n",
    "        area1 = (box1[2] - box1[0] + 1) * (box1[3] - box1[1] + 1)\n",
    "    \n",
    "    if area2 is None:\n",
    "        #Area of the box 2. +1 since index starts from 0.\n",
    "        area2 = (box2[2] - box2[0] + 1) * (box2[3] - box2[1] + 1)\n",
    "    \n",
    "    #Determine the intersection rectangle.\n",
    "    int_rect_top_left_x = max(box1[0], box2[0])\n",
    "    int_rect_top_left_y = max(box1[1], box2[1])\n",
    "    int_rect_btm_rght_x = min(box1[2], box2[2])\n",
    "    int_rect_btm_rght_y = min(box1[3], box2[3])\n",
    "    \n",
    "    #if the boxes do not intersect, the difference will be < 0. Hence we pick 0 in those cases.\n",
    "    int_rect_area = max(0, int_rect_btm_rght_x - int_rect_top_left_x + 1)*max(0, int_rect_btm_rght_y - int_rect_top_left_y)\n",
    "    \n",
    "    #Calculate the IoU.\n",
    "    intersect_over_union = float(int_rect_area / (area1 + area2 - int_rect_area))\n",
    "    \n",
    "    return intersect_over_union\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_label(class_labels, ground_truth_boxes, anchors, anchor_booleans, num_class=num_of_class,\n",
    "                    neg_anchor_thresh = neg_threshold, pos_anchor_thresh = pos_threshold):\n",
    "    '''\n",
    "    Each and every anchor that was generated (except for the ignored ones) must be labelled with either positive or negative depending on how\n",
    "    much the particular anchor box intersects with a ground-truth box. If an anchor is positive, the difference between the anchor's [x,y,w,h] and the\n",
    "    ground-truth box's [x,y,w,h] must be calculated in order for the network to learn how much to regress later on.\n",
    "\n",
    "    Input  : Classes, Ground truth box(es) belonging to one image [top-left-x, top-left-y, btm-right-x, btm-right-y], all the anchors and anchor booleans.\n",
    "    Output : Anchor Booleans (to know which anchor to ignore), Objectness array, Coordinate difference array.\n",
    "    '''\n",
    "\n",
    "\n",
    "    number_of_anchors = len(anchors) #Get the total number of anchors.\n",
    "\n",
    "    #For every anchor, we want a 1-D array that denotes whether the anchor should contribute to the loss or not. By default, all of them contributes.\n",
    "    #Since we already have the anchor booleans list, we will convert that into a numpy array and reshape it accordingly.\n",
    "    anchor_boolean_array   = np.reshape(np.asarray(anchor_booleans),(number_of_anchors, 1))\n",
    "    #For every anchor, we want a 2-D array that denotes whether the IoU of the anchor with a ground-truth object is more than certain threshold or not.\n",
    "    objectness_label_array = np.zeros((number_of_anchors, 2), dtype=np.float32)\n",
    "    #For every anchor, we want a 4-D array that denotes how much the anchor should regress in order to fit an object. (Only for positive anchors)\n",
    "    box_regression_array   = np.zeros((number_of_anchors, 4), dtype=np.float32)\n",
    "    #For every anchor, we want a num_of_class-D array that denotes which class does the object belongs to (Only if the anchor is positive)\n",
    "    class_array            = np.zeros((number_of_anchors, num_class), dtype=np.float32)\n",
    "\n",
    "    #We want to iterate through every ground truth box.\n",
    "    for j in range(ground_truth_boxes.shape[0]):\n",
    "\n",
    "        #get the class label\n",
    "        class_label = class_labels[j]\n",
    "\n",
    "        #Get the ground truth box's coordinates.\n",
    "        gt_box_top_left_x = ground_truth_boxes[j][0]\n",
    "        gt_box_top_left_y = ground_truth_boxes[j][1]\n",
    "        gt_box_btm_rght_x = ground_truth_boxes[j][2]\n",
    "        gt_box_btm_rght_y = ground_truth_boxes[j][3]\n",
    "\n",
    "        #Calculate the area of the original bounding box.1 is added since the index starts from 0 not 1.\n",
    "        gt_box_area = (gt_box_btm_rght_x - gt_box_top_left_x + 1)*(gt_box_btm_rght_y - gt_box_top_left_y + 1)\n",
    "\n",
    "        #Loop through the anchors.\n",
    "        for i in range(number_of_anchors):\n",
    "\n",
    "            #Check if the anchor should be ignored or not. If it is to be ignored, skip this i-th loop.\n",
    "            if int(anchor_boolean_array[i][0]) == 0:\n",
    "                continue\n",
    "\n",
    "            anchor = anchors[i] #Select the i-th anchor [x,y,w,h]\n",
    "\n",
    "            #Since our anchors are in [x,y,w,h] format, we want to convert them to the [top-left-x, top-left-y, btm-right-x, btm-right-y] first.\n",
    "            anchor_top_left_x = anchor[0] - anchor[2]/2\n",
    "            anchor_top_left_y = anchor[1] - anchor[3]/2\n",
    "            anchor_btm_rght_x = anchor[0] + anchor[2]/2\n",
    "            anchor_btm_rght_y = anchor[1] + anchor[3]/2\n",
    "\n",
    "            #Get the area of the bounding box.\n",
    "            anchor_box_area = (anchor_btm_rght_x - anchor_top_left_x + 1)*\\\n",
    "                                    (anchor_btm_rght_y - anchor_top_left_y + 1)\n",
    "\n",
    "            #wrap up both the ground truth box and the anchor box in lists.\n",
    "            box_1 = [gt_box_top_left_x, gt_box_top_left_y, gt_box_btm_rght_x, gt_box_btm_rght_y]\n",
    "            box_2 = [anchor_top_left_x, anchor_top_left_y, anchor_btm_rght_x, anchor_btm_rght_y]\n",
    "            \n",
    "            #Get the IoU.\n",
    "            intersect_over_union = IoU_calculator(box_1, box_2, gt_box_area, anchor_box_area)\n",
    "            \n",
    "            \n",
    "            #If the IoU is above or equal to the set threshold, then we want to label it as a positive anchor.\n",
    "            #If it is lower or equal to the set threshold, we want to label it as a negative anchor.\n",
    "            #Normally, there are going to be more negative anchors than the positive ones.\n",
    "            #In an image, there might be more than 1 object. Therefore, an anchor which was labelled as positive for the first object\n",
    "            #can be labelled as negative for the second object. We do not want the positive anchors to be overwritten since positive anchors\n",
    "            #are naturally lower in number. Therefore, if an anchor is already labelled positively, we're not going to label it as negative nor neutral\n",
    "            #on the following objects.\n",
    "            if intersect_over_union >= pos_anchor_thresh:\n",
    "\n",
    "\n",
    "                objectness_label_array[i][0] = 1.0 #positive label is on the left\n",
    "                objectness_label_array[i][1] = 0.0 #overwrite the negative label in case this anchor is labelled negatively for previous object(s).\n",
    "\n",
    "                class_array[i][int(class_label)] = 1.0 #Denote the label of the class in the array.\n",
    "\n",
    "                ##################################################################\n",
    "                #We want to calculate the regression values for positive anchors.#\n",
    "                ##################################################################\n",
    "\n",
    "                #Get the ground-truth box's [x,y,w,h]\n",
    "                gt_box_center_x = ground_truth_boxes[j][0] + ground_truth_boxes[j][2]/2\n",
    "                gt_box_center_y = ground_truth_boxes[j][1] + ground_truth_boxes[j][3]/2\n",
    "                gt_box_width    = ground_truth_boxes[j][2] - ground_truth_boxes[j][0]\n",
    "                gt_box_height   = ground_truth_boxes[j][3] - ground_truth_boxes[j][1]\n",
    "\n",
    "                #Regression loss according to the paper.\n",
    "                delta_x = (gt_box_center_x - anchor[0])/anchor[2]\n",
    "                delta_y = (gt_box_center_y - anchor[1])/anchor[3]\n",
    "                delta_w = math.log(gt_box_width/anchor[2])\n",
    "                delta_h = math.log(gt_box_height/anchor[3])\n",
    "\n",
    "                #Fill in the calculated values in the array.\n",
    "                box_regression_array[i][0] = delta_x\n",
    "                box_regression_array[i][1] = delta_y\n",
    "                box_regression_array[i][2] = delta_w\n",
    "                box_regression_array[i][3] = delta_h\n",
    "\n",
    "            if intersect_over_union <= neg_anchor_thresh:\n",
    "\n",
    "                #Check if the anchor is already labelled positive or not.\n",
    "                if int(objectness_label_array[i][0]) == 0:\n",
    "\n",
    "                    objectness_label_array[i][1] = 1.0\n",
    "\n",
    "            #These are neutral anchors.\n",
    "            if intersect_over_union > neg_anchor_thresh and intersect_over_union < pos_anchor_thresh:\n",
    "\n",
    "                #We do not want to label either the negative or the positive anchors as neutral.\n",
    "                if int(objectness_label_array[i][0]) == 0 and int(objectness_label_array[i][1]) == 0:\n",
    "                    anchor_boolean_array[i][0] = 0.0 #Neutral anchors are to be ignored.\n",
    "\n",
    "\n",
    "    return anchor_boolean_array, objectness_label_array, box_regression_array, class_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anchor_sampling(anchor_booleans, objectness_label, anchor_sampling_amount=anchor_sampling_amount):\n",
    "\n",
    "    '''\n",
    "    Faster R-CNN randomly samples a fixed amount of negative anchors and positive anchors for training. If we use all the neg and pos anchors,\n",
    "    our model will overfit on the negative ones as negative anchors are larger in amount compared to the positive anchors.\n",
    "\n",
    "    Input : anchor booleans and objectness label\n",
    "    Output: Updated anchor booleans. \n",
    "\n",
    "    '''\n",
    "\n",
    "    positive_count = 0\n",
    "    negative_count = 0\n",
    "    \n",
    "\n",
    "    #Iterate through every label.\n",
    "    for i in range(objectness_label.shape[0]):\n",
    "\n",
    "        if int(objectness_label[i][0]) == 1: #If the anchor is positively labelled.\n",
    "\n",
    "            if positive_count > anchor_sampling_amount: #If the positive anchors are more than the threshold amount, set the boolean to 0.\n",
    "\n",
    "                anchor_booleans[i][0] = 0.0\n",
    "\n",
    "            positive_count += 1\n",
    "\n",
    "        if int(objectness_label[i][1]) == 1: #If the anchor is negatively labelled.\n",
    "\n",
    "            if negative_count > anchor_sampling_amount: #If the negative anchors are more than the threshold amount, set the boolean to 0.\n",
    "\n",
    "                anchor_booleans[i][0] = 0.0\n",
    "\n",
    "            negative_count += 1\n",
    "\n",
    "    #Return the updated booleans. REMEMBER! This array was passed by reference. \n",
    "    return anchor_booleans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_images(im_index):\n",
    "    '''\n",
    "    Read the image files, resize, normalize the images and return them in a numpy array.\n",
    "    Input : first and last index.\n",
    "    Output: Numpy array of images.\n",
    "    '''\n",
    "        \n",
    "    im = cv2.imread(list_images[im_index])\n",
    "    im = cv2.resize(im, (image_height, image_width))/255\n",
    "    \n",
    "    return np.asarray(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(first_index, last_index, anchors, anchor_booleans):\n",
    "        '''\n",
    "        This function generates the dataset using the functions defined earlier for the given number of data. If the write status is True,\n",
    "        the output of this function will be written on disk.\n",
    "\n",
    "        Input : starting index and final index of the dataset to be generated.\n",
    "        Output: Anchor booleans, Objectness Label and Regression Label in specified batches.\n",
    "\n",
    "        '''\n",
    "        num_of_anchors = len(anchors)\n",
    "        \n",
    "        batch_anchor_booleans   = []\n",
    "        batch_objectness_array  = []\n",
    "        batch_regression_array  = []\n",
    "        batch_class_label_array = []\n",
    "        batch_images_array      = []\n",
    "        batch_ground_truth_boxes= []\n",
    "\n",
    "        for i in range(first_index, last_index):\n",
    "\n",
    "            #Get the true labels and the ground truth boxes [x,y,w,h] for every file.\n",
    "            true_labels, ground_truth_boxes = get_labels_from_xml(xml_file_path=list_annotations[i])\n",
    "          \n",
    "            #Get the updated anchor booleans, objectness label and regression label.\n",
    "            anchor_bools, objectness_label_array, box_regression_array, class_array = generate_label(true_labels, ground_truth_boxes, \n",
    "                                                                                                        anchors, anchor_booleans)\n",
    "            \n",
    "            #If there are no positive anchors, the training data will be ignored.\n",
    "            if math.ceil(np.sum(objectness_label_array[:, 0])) == 0:\n",
    "                continue\n",
    "                \n",
    "            #Get the updated anchor bools based on the sampling.\n",
    "            anchor_bools = anchor_sampling(anchor_bools, objectness_label_array)\n",
    "            \n",
    "            batch_images_array.append(read_images(i))\n",
    "            batch_anchor_booleans.append(anchor_bools)\n",
    "            batch_objectness_array.append(objectness_label_array)\n",
    "            batch_regression_array.append(box_regression_array)\n",
    "            batch_ground_truth_boxes.append(ground_truth_boxes)\n",
    "            batch_class_label_array.append(class_array)\n",
    "            \n",
    "\n",
    "        batch_anchor_booleans   = np.reshape(np.asarray(batch_anchor_booleans), (-1,num_of_anchors))\n",
    "        batch_objectness_array  = np.asarray(batch_objectness_array)\n",
    "        batch_regression_array  = np.asarray(batch_regression_array)\n",
    "        batch_class_label_array = np.asarray(batch_class_label_array)\n",
    "        batch_ground_truth_boxes= np.asarray(batch_ground_truth_boxes)\n",
    "        batch_images_array      = np.asarray(batch_images_array)\n",
    "        \n",
    "\n",
    "        return (batch_images_array,batch_anchor_booleans, batch_objectness_array, batch_regression_array, batch_ground_truth_boxes, batch_class_label_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchors, an_bools = generate_anchors() #We only need to generate the anchors and the anchor booleans once.\n",
    "num_of_anchors = len(anchors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL BUILDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-5\n",
    "epoch = 1000\n",
    "batch_size = 10\n",
    "model_checkpoint = './model_ckpt/model.ckpt'\n",
    "decay_steps = 10000\n",
    "decay_rate = 0.99\n",
    "lambda_value = 10\n",
    "top_k_prediction = 5\n",
    "confidence_thresh = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_pred(logit, bb, top_k=top_k_prediction):\n",
    "    '''\n",
    "    Input logits and corresponding boundding box predictions.\n",
    "    Sort both the arrays in descending order (based on objectness confidence).\n",
    "    Returns the top-k of the sorted arrays.\n",
    "    '''\n",
    "    \n",
    "    #Returns the indexes of the logits based on objectness confidence in ascending order.\n",
    "    sorted_index = logit[:,:,0].argsort()\n",
    "    sorted_index = np.flip(sorted_index) #Flip the array to get the descending order indexes.\n",
    "\n",
    "    sorted_confidence = logit[:, sorted_index[0,:]] #sort the logits in descending order.\n",
    "    sorted_bbox       = bb[:,sorted_index[0,:]] #sort the bounding boxes in decreasing confidence order.\n",
    "    \n",
    "    top_confidence = sorted_confidence[:,:top_k] #get the top-k logits.\n",
    "    top_bounding_boxes = sorted_bbox[:,:top_k] #get the corresponding top-k bounding boxes.\n",
    "    \n",
    "    return top_confidence, top_bounding_boxes, sorted_index[:,:top_k] #return indexes sliced to top-k predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bounding_box(boxes, anchor_index, anchors=anchors):\n",
    "    '''\n",
    "    Since the regressed values are based off an anchor, in order to calculate the bounding box coordinates,\n",
    "    we need to get the reference anchor first. \n",
    "    Input the regression values and the anchor index.\n",
    "    Returns the bounding box coordinates referenced to the input image.\n",
    "    '''\n",
    "    bounding_box_coordinates = []\n",
    "\n",
    "    for i in range(boxes.shape[0]):\n",
    "\n",
    "        anchor_pick = anchors[anchor_index[i]]\n",
    "\n",
    "        x = (boxes[i][0]*anchor_pick[2]) + anchor_pick[0]\n",
    "        y = (boxes[i][1]*anchor_pick[3]) + anchor_pick[1]\n",
    "        w = anchor_pick[2]*(math.e**boxes[i][2])\n",
    "        h = anchor_pick[3]*(math.e**boxes[i][3])\n",
    "\n",
    "        top_left_x  = float(x - w/2)\n",
    "        top_left_y  = float(y - h/2)\n",
    "        btm_right_x = float(x + w/2)\n",
    "        btm_right_y = float(y + h/2)\n",
    "\n",
    "        bb_list = np.asarray([top_left_x, top_left_y, btm_right_x, btm_right_y])\n",
    "\n",
    "        bounding_box_coordinates.append(bb_list)\n",
    "            \n",
    "    \n",
    "    return np.asarray(bounding_box_coordinates)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_IoU(pred_bb, sorted_index, true_bb):\n",
    "    '''\n",
    "    Input predicted regression value, the index of the anchor, and the true bounding boxes.\n",
    "    1) Calculate the coordinate of the predicted bounding boxes based on the anchor that the regression value is\n",
    "       derived from (the index of the anchor is the second parameter provided).\n",
    "    2) Calculate the IoU of every predicted bounding box against every ground truth box and label the predicted\n",
    "       bounding boxes with > 0.5 IoU as positive and negative otherwise.\n",
    "    \n",
    "    Returns an array that represents the label (positive or negative) for each predicted bounding box.\n",
    "    '''\n",
    "    \n",
    "    #SHAPES\n",
    "    # pred_bb      - [top_k_prediction, 4]\n",
    "    # sorted_index - [top_k_prediction]\n",
    "    # true_bb      - [num_of_anchors, 4]\n",
    "    \n",
    "    #get the bounding boxes coordinates\n",
    "    pred_boxes = get_bounding_box(pred_bb, sorted_index)\n",
    "    \n",
    "    IoU_marked_arrays = np.zeros((pred_boxes.shape[0]))\n",
    "    \n",
    "    #iterate through every bounding box.\n",
    "    for bounding_box in true_bb:\n",
    "        ind = 0\n",
    "        #iterate through every predicted boxes.\n",
    "        flag = False #for every ground truth box, we only want 1 predicted box with the highest confidence.\n",
    "        for pred_box in pred_boxes:\n",
    "            \n",
    "            if flag:\n",
    "                continue\n",
    "            #get the IoU of the two boxes\n",
    "            iou = IoU_calculator(bounding_box, pred_box)\n",
    "            \n",
    "            #if the IoU is above the threshold.\n",
    "            if iou > 0.5:\n",
    "                IoU_marked_arrays[ind] = 1.0\n",
    "                flag = True\n",
    "            \n",
    "            ind += 1 #we don't care if a predicted box overlaps more than one gt box.\n",
    "    \n",
    "    return IoU_marked_arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_AP(top_logits, top_pred_bb, true_bb, sorted_index):\n",
    "    '''\n",
    "    Input the top-k confidences, predicted regression values and the ground truth boxes, the index the\n",
    "    confidence and predicted bounding boxes obtained from.\n",
    "    1) Check IoU of the predicted boxes against the Ground-Truth boxes.\n",
    "    2) Determine if a predicted box is a TP, FP or FN.\n",
    "    3) Calculate Precision and Recall for each prediction.\n",
    "    4) Calculate the Interpolated Precisions.\n",
    "    5) Calculate the Area Under Curve to get the Average Precision (AP). \n",
    "    6) Since there are only one class (every image has at least 1 object, hence only 1 class), there is no \n",
    "       need for mAP. Only AP.\n",
    "    '''\n",
    "    \n",
    "    ap_list = []\n",
    "    for batch_index in range (top_logits.shape[0]): #Iterate through every prediction in a batch\n",
    "        \n",
    "        #calculate the IoU of every predicted box with every ground-truth box.\n",
    "        IoU_array = calculate_IoU(top_pred_bb[batch_index], sorted_index[batch_index], true_bb[batch_index])\n",
    "        \n",
    "        \n",
    "        total_ground_truth_box = true_bb[batch_index].shape[0] #to calculate recall\n",
    "        #iterate through every predicted box\n",
    "        \n",
    "        counter, tp, fp, fn = 0, 0, 0, 0\n",
    "        \n",
    "        precision_list, recall_list = [],[]\n",
    "        \n",
    "        for index in range(top_logits.shape[1]):\n",
    "            \n",
    "            #if both the IOU and the confidence are above the set threshold. (True Positive)\n",
    "            if int(IoU_array[index]) == 1 and top_logits[batch_index,index,0] > confidence_thresh :\n",
    "\n",
    "                tp += 1\n",
    "                counter += 1\n",
    "                precision = tp/counter\n",
    "                recall = tp/total_ground_truth_box\n",
    "                precision_list.append(precision)\n",
    "                recall_list.append(recall)\n",
    "            \n",
    "            #if the confidence is above threshold, but IoU is below threshold. (False Positive)\n",
    "            if int(IoU_array[index]) == 0 and top_logits[batch_index,index,0] > confidence_thresh :\n",
    "                \n",
    "                fp += 1\n",
    "                counter +=1 \n",
    "                precision = tp/counter\n",
    "                recall = tp/total_ground_truth_box\n",
    "                precision_list.append(precision)\n",
    "                recall_list.append(recall)\n",
    "            \n",
    "            #if the IoU is above threshold but the confidence is below threshold. (False Negative)\n",
    "            if int(IoU_array[index]) == 1 and top_logits[batch_index,index,0] <= confidence_thresh : \n",
    "                \n",
    "                fn += 1\n",
    "                counter +=1\n",
    "                precision = tp/counter\n",
    "                recall = tp/total_ground_truth_box\n",
    "                precision_list.append(precision)\n",
    "                recall_list.append(recall)\n",
    "        \n",
    "        #Reverse the precision list and convert it into np array\n",
    "        reversed_precision = np.asarray(precision_list[::-1])\n",
    "        interpolated_precision = np.flip(np.maximum.accumulate(reversed_precision))\n",
    "        \n",
    "        summation = 0\n",
    "        for recall_index in range(len(recall_list) - 1):\n",
    "            \n",
    "            #Calculate Area Under Curve.\n",
    "            curr_recall = recall_list[recall_index]\n",
    "            next_recall = recall_list[recall_index+1]\n",
    "            ip = interpolated_precision[recall_index]\n",
    "            \n",
    "            current_summation = (next_recall - curr_recall)*(ip)\n",
    "            summation += current_summation\n",
    "        \n",
    "        AP = summation\n",
    "        ap_list.append(AP)\n",
    "    \n",
    "    #Calculate the mean across all the APs in the batch (NOT mAP!)\n",
    "    batch_average_AP = sum(ap_list)/top_logits.shape[0]\n",
    "        \n",
    "    return batch_average_AP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_func(t):\n",
    "    \n",
    "    t = tf.abs(t)\n",
    "    \n",
    "    comparison_tensor = tf.ones((num_of_anchors, 4))\n",
    "    smoothed = tf.where(tf.less(t, comparison_tensor), 0.5*tf.pow(t,2), t - 0.5)\n",
    "    \n",
    "    return smoothed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_L1(pred_box, truth_box):\n",
    "    \n",
    "    diff = pred_box - truth_box\n",
    "    \n",
    "    smoothed = tf.map_fn(smooth_func, diff)\n",
    "    \n",
    "    return smoothed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X       = tf.placeholder(tf.float32, shape=(None, image_height, image_width, image_depth)) \n",
    "Y_obj   = tf.placeholder(tf.float32, shape=(None, num_of_anchors,2))\n",
    "Y_coor  = tf.placeholder(tf.float32, shape=(None, num_of_anchors,4))\n",
    "anch_bool = tf.placeholder(tf.float32, shape=(None, num_of_anchors))\n",
    "\n",
    "conv1 = tf.contrib.layers.conv2d(X, num_outputs=64, kernel_size=3, stride=1, \n",
    "                                 padding='SAME', activation_fn=tf.nn.relu)\n",
    "conv2 = tf.contrib.layers.conv2d(conv1, num_outputs=64, kernel_size=3, stride=1, \n",
    "                                 padding='SAME', activation_fn=tf.nn.relu)\n",
    "conv2_pool = tf.nn.max_pool(conv2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "\n",
    "\n",
    "conv3 = tf.contrib.layers.conv2d(conv2_pool, num_outputs=128, kernel_size=3, stride=1, \n",
    "                                 padding='SAME', activation_fn=tf.nn.relu)\n",
    "conv4 = tf.contrib.layers.conv2d(conv3, num_outputs=128, kernel_size=3, stride=1, \n",
    "                                 padding='SAME', activation_fn=tf.nn.relu)\n",
    "conv4_pool = tf.nn.max_pool(conv4, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "\n",
    "conv5 = tf.contrib.layers.conv2d(conv4_pool, num_outputs=512, kernel_size=3, stride=1, \n",
    "                                 padding='SAME', activation_fn=tf.nn.relu)\n",
    "conv6 = tf.contrib.layers.conv2d(conv5, num_outputs=256, kernel_size=3, stride=1, \n",
    "                                 padding='SAME', activation_fn=tf.nn.relu)\n",
    "conv7 = tf.contrib.layers.conv2d(conv6, num_outputs=256, kernel_size=3, stride=1, \n",
    "                                 padding='SAME', activation_fn=tf.nn.relu)\n",
    "conv7_pool = tf.nn.max_pool(conv7, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpn_conv = tf.contrib.layers.conv2d(conv7_pool, num_outputs=512, kernel_size=3, stride=1, \n",
    "                                    padding='VALID', activation_fn=tf.nn.relu)\n",
    "\n",
    "obj_conv = tf.contrib.layers.conv2d(rpn_conv, num_outputs=18, kernel_size=1, stride=1, padding='VALID', activation_fn=None)\n",
    "bb_conv = tf.contrib.layers.conv2d(rpn_conv, num_outputs=36, kernel_size=1, stride=1, padding='VALID', activation_fn=None)\n",
    "\n",
    "class_conv_reshape = tf.reshape(obj_conv, (-1, num_of_anchors, 2))\n",
    "anchor_conv_reshape = tf.reshape(bb_conv, (-1, num_of_anchors, 4))\n",
    "\n",
    "logits = tf.nn.softmax(class_conv_reshape)\n",
    "\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "decayed_lr = tf.train.exponential_decay(learning_rate,\n",
    "                                            global_step, decay_steps,\n",
    "                                            decay_rate, staircase=True)\n",
    "\n",
    "\n",
    "loss1 = 1/256*tf.reduce_sum(anch_bool*(tf.nn.softmax_cross_entropy_with_logits(labels=Y_obj, logits=class_conv_reshape)))\n",
    "loss2 = lambda_value*(1/128)*tf.reduce_sum((tf.reshape(Y_obj[:,:,0], (-1,num_of_anchors,1)))*smooth_L1(anchor_conv_reshape, Y_coor))\n",
    "\n",
    "total_loss = loss1 + loss2\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(decayed_lr).minimize(total_loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "try:\n",
    "    saver.restore(sess, model_checkpoint)\n",
    "    print(\"Model has been loaded!\")\n",
    "    \n",
    "except:\n",
    "    \n",
    "    print(\"Model is not loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAINING \n",
    "\n",
    "for epoch_idx in range(epoch): #Each epoch.\n",
    "    \n",
    "    \n",
    "    total_loss = 0\n",
    "    total_ap = 0\n",
    "    #Loop through the whole dataset in batches.\n",
    "    counter = 0\n",
    "    for start_idx in tqdm(range(0, total_images, batch_size)):\n",
    "        \n",
    "        end_idx = start_idx + batch_size\n",
    "        \n",
    "        if end_idx >= total_images : end_idx = total_images - 1 #In case the end index exceeded the dataset.\n",
    "            \n",
    "        \n",
    "        #Get the labels needed.\n",
    "        images, batch_anchor_booleans, batch_objectness_array, batch_regression_array, batch_gt_boxes, _ = \\\n",
    "                                                generate_dataset(start_idx,end_idx, anchors, an_bools)\n",
    "        \n",
    "        #if there are no data\n",
    "        if images.shape[0] == 0:\n",
    "            continue\n",
    "            \n",
    "        #Optimize the model.\n",
    "        _, theloss, confidences, pred_bb = sess.run([optimizer, total_loss, logits, anchor_conv_reshape], \n",
    "                                                        feed_dict={X: images,\n",
    "                                                                  Y_obj:batch_objectness_array,\n",
    "                                                                  Y_coor: batch_regression_array,\n",
    "                                                                  anch_bool: batch_anchor_booleans})        \n",
    "        \n",
    "        \n",
    "        top_logits, top_boxes, sorted_indexes = top_k_pred(confidences, pred_bb)\n",
    "        \n",
    "        avg_precision = get_AP(top_logits, top_boxes, batch_gt_boxes, sorted_indexes)\n",
    "        \n",
    "        #keep track of the measurements\n",
    "        total_loss += theloss\n",
    "        total_ap += avg_precision\n",
    "        counter +=1\n",
    "\n",
    "        \n",
    "    #Save the model periodically.\n",
    "    if epoch%5 == 0:\n",
    "        saver.save(sess, model_checkpoint)\n",
    "    \n",
    "    print(\"Epoch : %d, Loss : %g, Avg. Prec : %g\"%(epoch_idx, total_loss, total_ap/counter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POST PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_im = read_images(5,6) #read a sample image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(sample_im[0])\n",
    "plt.title(\"Sample Image\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence, boxes = sess.run([logits, anchor_conv_reshape], feed_dict={X:sample_im}) #get the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.ones((num_of_anchors)) - (1 - confidence_thresh) #this mask is used to filter out the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_list = np.where(confidence[:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nasa",
   "language": "python",
   "name": "nasa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
