{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Region Proposal Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Program Flow**\n",
    "\n",
    "1) Preprocess data to fit the format of Faster R-CNN.\n",
    "   - Extract information from the image and label files of Pascal VOC.\n",
    "   - Generate anchors.\n",
    "   - Keep track of list of anchors that should be ignored (out of boundary anchors).\n",
    "   - Determine positive and negative anchors for each image data.\n",
    "   - Determine the difference between positive anchors and the corresponding ground-truth objects based on the given formula.\n",
    "   - Sample anchors for training (by using the list to keep track of ignored anchors earlier).\n",
    "\n",
    "2) Build RPN model.\n",
    "    - Make sure the kernel size selected is the same with the kernel size used when generating anchors earlier.\n",
    "    - Make sure the ignored anchors do not contribute to loss.\n",
    "    \n",
    "3) Post-Processing.\n",
    "    - Choose prediction anchors that are above a certain confidence level to represent the bounding boxes.\n",
    "    - Use the correct anchors (generated ones) to apply the regression results to. \n",
    "    - Use Non-Max Suppression technique to filter out multiple bounding box predictions on one object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/topiary/nasa/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/topiary/nasa/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/topiary/nasa/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:521: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/topiary/nasa/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:522: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/topiary/nasa/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/topiary/nasa/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import xmltodict\n",
    "import tensorflow as tf\n",
    "import math\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_images_path     = '../../VOCdevkit/VOC2012/JPEGImages'\n",
    "data_annotation_path = '../../VOCdevkit/VOC2012/Annotations'\n",
    "image_height = 224\n",
    "image_width  = 224\n",
    "image_depth  = 3\n",
    "rpn_kernel_size = 3\n",
    "subsampled_ratio = 8\n",
    "anchor_sizes = [32,64,128]\n",
    "anchor_aspect_ratio = [[1,1],[1,2],[2,1]]\n",
    "num_anchors_in_box = len(anchor_sizes)*len(anchor_aspect_ratio)\n",
    "neg_threshold = 0.3\n",
    "pos_threshold = 0.7\n",
    "anchor_sampling_amount = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the image and annotation file paths\n",
    "list_images      = sorted([x for x in glob.glob(data_images_path + '/**')])     #length : 17125\n",
    "list_annotations = sorted([x for x in glob.glob(data_annotation_path + '/**')]) #length : 17125\n",
    "total_images = len(list_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classes(xml_files=list_annotations):\n",
    "    '''\n",
    "    Output: All the distinct classes for this dataset.\n",
    "    \n",
    "    '''\n",
    "    classes = []\n",
    "    \n",
    "    for file in xml_files: \n",
    "\n",
    "        f = open(file)\n",
    "        doc = xmltodict.parse(f.read()) #parse the xml file to python dict.\n",
    "\n",
    "        #Images in the dataset might contain either 1 object or more than 1 object. For images with 1 object, the annotation for the object\n",
    "        #in the xml file will be located in 'annotation' -> 'object' -> 'name'. For images with more than 1 object, the annotations for the objects\n",
    "        #will be nested in 'annotation' -> 'object' thus requiring a loop to iterate through them. (Pascal VOC format)\n",
    "\n",
    "        try: \n",
    "            #try iterating through the tag. (For images with more than 1 obj.)\n",
    "            for obj in doc['annotation']['object']:\n",
    "                classes.append(obj['name'].lower()) #append the lowercased string.\n",
    "\n",
    "        except TypeError as e: #iterating through non-nested tags would throw a TypeError.\n",
    "            classes.append(doc['annotation']['object']['name'].lower()) #append the lowercased string.\n",
    "\n",
    "        f.close()\n",
    "\n",
    "    classes = list(set(classes)) #remove duplicates.\n",
    "    classes.sort()\n",
    "\n",
    "    #returns a list containing the names of classes after being sorted.\n",
    "    return classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = get_classes()\n",
    "num_of_class = len(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels_from_xml(xml_file_path, num_of_class = num_of_class):\n",
    "    '''\n",
    "    Input : A SINGLE xml file and the total number of classes in the dataset. \n",
    "    Output: Labels in numpy array format (Object classes their corresponding bounding box coordinates).\n",
    "\n",
    "    Desc : This function parses a single xml file and outputs the objects classes and their corresponding bounding box coordinates\n",
    "           [top-left-x, top-left-y, btm-right-x, btm-right-y] on the resized image.\n",
    "\n",
    "    '''\n",
    "\n",
    "    f = open(xml_file_path)\n",
    "    doc = xmltodict.parse(f.read()) #parse the xml file to python dict.\n",
    "\n",
    "    #get the original image height and width. Images have different height and width from each other.\n",
    "    ori_img_height = float(doc['annotation']['size']['height'])\n",
    "    ori_img_width  = float(doc['annotation']['size']['width'])\n",
    "\n",
    "\n",
    "    class_label = [] #init for keeping track objects' labels.\n",
    "    bbox_label  = [] #init for keeping track of objects' bounding box (bb).\n",
    "\n",
    "\n",
    "    #Images in the dataset might contain either 1 object or more than 1 object. For images with 1 object, the annotation for the object\n",
    "    #in the xml file will be located in 'annotation' -> 'object' -> 'name'. For images with more than 1 object, the annotations for the objects\n",
    "    #will be nested in 'annotation' -> 'object' thus requiring a loop to iterate through them. (Pascal VOC format)\n",
    "    try:\n",
    "        #Try iterating through the tag (For images with more than 1 obj).\n",
    "        for each_obj in doc['annotation']['object']:\n",
    "\n",
    "            obj_class = each_obj['name'].lower() #get the label for the object and lowercase the string.\n",
    "\n",
    "            #Pascal VOC's format to denote bounding boxes are to denote the top left part of the box and the bottom right of the box.\n",
    "            #the coordinates are in terms of x and y axis for both part of the box.\n",
    "            x_min = float(each_obj['bndbox']['xmin']) #top left x-axis coordinate.\n",
    "            x_max = float(each_obj['bndbox']['xmax']) #bottom right x-axis coordinate.\n",
    "            y_min = float(each_obj['bndbox']['ymin']) #top left y-axis coordinate.\n",
    "            y_max = float(each_obj['bndbox']['ymax']) #bottom right y-axis coordinate.\n",
    "\n",
    "        ##################################################################################\n",
    "        #We want to make sure the coordinates are resized according to the resized image.#\n",
    "        ##################################################################################\n",
    "\n",
    "            #All the images will be resized to a fixed size in order to be fixed-size inputs to the neural network model.\n",
    "            #Therefore, we need to resize the coordinates as well since the coordinates above is based on the original size of the images.\n",
    "\n",
    "            #In order to find the resized coordinates, we must multiply the ratio of the resized image compared to its original to the coordinates.\n",
    "            x_min = float((image_width/ori_img_width)*x_min)\n",
    "            y_min = float((image_height/ori_img_height)*y_min)\n",
    "            x_max = float((image_width/ori_img_width)*x_max)\n",
    "            y_max = float((image_height/ori_img_height)*y_max)\n",
    "\n",
    "            generated_box_info = [x_min, y_min, x_max, y_max]\n",
    "\n",
    "\n",
    "            index = classes.index(obj_class) #get the index of the object's class.\n",
    "\n",
    "            #append each object's class label and the bounding box label (converted to Faster R-CNN format) into the list initialized earlier.\n",
    "            class_label.append(index)\n",
    "            bbox_label.append(np.asarray(generated_box_info, dtype='float32'))\n",
    "\n",
    "    except TypeError as e : #happens when the iteration through the tag fails due to only 1 object being in the image.\n",
    "\n",
    "        #SAME PROCEDURE AS ABOVE !  \n",
    "\n",
    "        #Getting these information from the XML file differs compared to above,\n",
    "        obj_class = doc['annotation']['object']['name']\n",
    "        x_min = float(doc['annotation']['object']['bndbox']['xmin']) \n",
    "        x_max = float(doc['annotation']['object']['bndbox']['xmax']) \n",
    "        y_min = float(doc['annotation']['object']['bndbox']['ymin']) \n",
    "        y_max = float(doc['annotation']['object']['bndbox']['ymax']) \n",
    "\n",
    "        x_min = float((image_width/ori_img_width)*x_min)\n",
    "        y_min = float((image_height/ori_img_height)*y_min)\n",
    "        x_max = float((image_width/ori_img_width)*x_max)\n",
    "        y_max = float((image_height/ori_img_height)*y_max)\n",
    "\n",
    "        generated_box_info = [x_min, y_min, x_max, y_max]\n",
    "\n",
    "        #Get the index of the class\n",
    "        index = classes.index(obj_class) \n",
    "\n",
    "        class_label.append(index)\n",
    "        bbox_label.append(np.asarray(generated_box_info, dtype='float32'))\n",
    "\n",
    "\n",
    "    return class_label, np.asarray(bbox_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_anchors(rpn_kernel_size=rpn_kernel_size, subsampled_ratio=subsampled_ratio,\n",
    "                     anchor_sizes=anchor_sizes, anchor_aspect_ratio=anchor_aspect_ratio):\n",
    "\n",
    "    '''\n",
    "    Since Faster R-CNN works by placing different sized anchors throughout an image on different positions, we need to generate the coordinates\n",
    "    for each of those anchors first before generating the labels required by Faster R-CNN. This function generates the anchors based on number\n",
    "    of anchors, sizes of anchors, aspect ratios of anchors, RPN's kernel size and subsampled size of the image.\n",
    "\n",
    "    Output : list of anchors (each anchor is denoted by (x,y,w,h)) and the list if an anchor should be ignored or not.\n",
    "\n",
    "    '''\n",
    "\n",
    "    list_of_anchors = []\n",
    "    anchor_booleans = [] #This is to keep track of an anchor's status. Anchors that are out of boundary are meant to be ignored.\n",
    "\n",
    "    #get the feature map's height and width after convolutions and poolings.\n",
    "\n",
    "\n",
    "    #the anchor's center is always at the middle grid of the RPN kernel. Therefore, we need to imitate the kernel slide to get all the\n",
    "    #centers of the anchor. If a kernel is 3x3, the center will start from (1,1). If a kernel is 5x5, the center will start from (2,2).\n",
    "    #we use divmod to get the first coordinate of the RPN kernel's middle point.\n",
    "    starting_center = divmod(rpn_kernel_size, 2)[0]\n",
    "\n",
    "    anchor_center = [starting_center - 1,starting_center] #-1 on the x-coor because the increment comes first in the while loop below.\n",
    "\n",
    "    #We want to imitate the kernel sliding with stride 1 until it reaches the ending center. Since index starts with 0, we subtract 1 more to the \n",
    "    #width and height of the subsampled image.\n",
    "    subsampled_height = image_height/subsampled_ratio\n",
    "    subsampled_width = image_width/subsampled_ratio\n",
    "    \n",
    "    while (anchor_center != [subsampled_width - (1 + starting_center), subsampled_height - (1 + starting_center)]): \n",
    "\n",
    "\n",
    "        anchor_center[0] += 1 #Increment the x-axis by 1.\n",
    "\n",
    "        #If the sliding window has reached the last central point at the right side, increase the y-axis by 1 and \n",
    "        #reset x-axis to 0.\n",
    "        if anchor_center[0] > subsampled_width - (1 + starting_center):\n",
    "\n",
    "            anchor_center[1] += 1\n",
    "            anchor_center[0] = starting_center\n",
    "\n",
    "        #Even though we calculate the anchor on the feature map, the anchors are still referenced to the original image. Therefore, \n",
    "        #once we obtain the position of the center of the anchor on the feature map, we multiply is by the downsampling ratio to obtain its \n",
    "        #center position referenced to the original input image.\n",
    "        anchor_center_on_image = [anchor_center[0]*subsampled_ratio, anchor_center[1]*subsampled_ratio]\n",
    "\n",
    "        #We want to calculate the anchor's height and width on all the different variations of aspect ratio and sizes.\n",
    "        #Iterate through every size defined for each anchor center's position.\n",
    "        for size in anchor_sizes:\n",
    "\n",
    "            #Iterate through every aspect ratio for each size.\n",
    "            for a_ratio in anchor_aspect_ratio:\n",
    "\n",
    "                #[x,y,w,h] of an anchor.\n",
    "                anchor_info = [anchor_center_on_image[0], anchor_center_on_image[1], size*a_ratio[0], size*a_ratio[1]]\n",
    "\n",
    "                #Perform check if a given anchor crosses the boundary of the image or not. Such anchors are to be ignored and will be labelled as 0.\n",
    "                #Else the anchor will be labelled as 1 (meaning good to go).\n",
    "                if (anchor_info[0] - anchor_info[2]/2 < 0 or anchor_info[0] + anchor_info[2]/2 > image_width or \n",
    "                                        anchor_info[1] - anchor_info[3]/2 < 0 or anchor_info[1] + anchor_info[3]/2 > image_height) :\n",
    "\n",
    "                    anchor_booleans.append([0.0])\n",
    "\n",
    "                else:\n",
    "\n",
    "                    anchor_booleans.append([1.0])\n",
    "\n",
    "                list_of_anchors.append(anchor_info)\n",
    "\n",
    "    return list_of_anchors, anchor_booleans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_label(class_labels, ground_truth_boxes, anchors, anchor_booleans, num_class=num_of_class,\n",
    "                    neg_anchor_thresh = neg_threshold, pos_anchor_thresh = pos_threshold):\n",
    "    '''\n",
    "    Each and every anchor that was generated (except for the ignored ones) must be labelled with either positive or negative depending on how\n",
    "    much the particular anchor box intersects with a ground-truth box. If an anchor is positive, the difference between the anchor's [x,y,w,h] and the\n",
    "    ground-truth box's [x,y,w,h] must be calculated in order for the network to learn how much to regress later on.\n",
    "\n",
    "    Input  : Classes, Ground truth box(es) belonging to one image [top-left-x, top-left-y, btm-right-x, btm-right-y], all the anchors and anchor booleans.\n",
    "    Output : Anchor Booleans (to know which anchor to ignore), Objectness array, Coordinate difference array.\n",
    "    '''\n",
    "\n",
    "\n",
    "    number_of_anchors = len(anchors) #Get the total number of anchors.\n",
    "\n",
    "    #For every anchor, we want a 1-D array that denotes whether the anchor should contribute to the loss or not. By default, all of them contributes.\n",
    "    #Since we already have the anchor booleans list, we will convert that into a numpy array and reshape it accordingly.\n",
    "    anchor_boolean_array   = np.reshape(np.asarray(anchor_booleans),(number_of_anchors, 1))\n",
    "    #For every anchor, we want a 2-D array that denotes whether the IoU of the anchor with a ground-truth object is more than certain threshold or not.\n",
    "    objectness_label_array = np.zeros((number_of_anchors, 2), dtype=np.float32)\n",
    "    #For every anchor, we want a 4-D array that denotes how much the anchor should regress in order to fit an object. (Only for positive anchors)\n",
    "    box_regression_array   = np.zeros((number_of_anchors, 4), dtype=np.float32)\n",
    "    #For every anchor, we want a num_of_class-D array that denotes which class does the object belongs to (Only if the anchor is positive)\n",
    "    class_array            = np.zeros((number_of_anchors, num_class), dtype=np.float32)\n",
    "\n",
    "    #We want to iterate through every ground truth box.\n",
    "    for j in range(ground_truth_boxes.shape[0]):\n",
    "\n",
    "        #get the class label\n",
    "        class_label = class_labels[j]\n",
    "\n",
    "        #Get the ground truth box's coordinates.\n",
    "        gt_box_top_left_x = ground_truth_boxes[j][0]\n",
    "        gt_box_top_left_y = ground_truth_boxes[j][1]\n",
    "        gt_box_btm_rght_x = ground_truth_boxes[j][2]\n",
    "        gt_box_btm_rght_y = ground_truth_boxes[j][3]\n",
    "\n",
    "        #Calculate the area of the original bounding box.1 is added since the index starts from 0 not 1.\n",
    "        gt_box_area = (gt_box_btm_rght_x - gt_box_top_left_x + 1)*(gt_box_btm_rght_y - gt_box_top_left_y + 1)\n",
    "\n",
    "        #Loop through the anchors.\n",
    "    \n",
    "        for i in range(number_of_anchors):\n",
    "\n",
    "            #Check if the anchor should be ignored or not. If it is to be ignored, skip this i-th loop.\n",
    "            if int(anchor_boolean_array[i][0]) == 0:\n",
    "\n",
    "                continue\n",
    "\n",
    "            anchor = anchors[i] #Select the i-th anchor [x,y,w,h]\n",
    "\n",
    "            #Since our anchors are in [x,y,w,h] format, we want to convert them to the [top-left-x, top-left-y, btm-right-x, btm-right-y] first.\n",
    "            anchor_top_left_x = anchor[0] - anchor[2]/2\n",
    "            anchor_top_left_y = anchor[1] - anchor[3]/2\n",
    "            anchor_btm_rght_x = anchor[0] + anchor[2]/2\n",
    "            anchor_btm_rght_y = anchor[1] + anchor[3]/2\n",
    "\n",
    "            #Get the area of the bounding box.\n",
    "            anchor_box_area = (anchor_btm_rght_x - anchor_top_left_x + 1)*(anchor_btm_rght_y - anchor_top_left_y + 1)\n",
    "\n",
    "            #Determine the intersection rectangle.\n",
    "            int_rect_top_left_x = max(gt_box_top_left_x, anchor_top_left_x)\n",
    "            int_rect_top_left_y = max(gt_box_top_left_y, anchor_top_left_y)\n",
    "            int_rect_btm_rght_x = min(gt_box_btm_rght_x, anchor_btm_rght_x)\n",
    "            int_rect_btm_rght_y = min(gt_box_btm_rght_y, anchor_btm_rght_y)\n",
    "\n",
    "            #if the boxes do not intersect, the difference will be < 0. Hence we pick 0 in those cases.\n",
    "            int_rect_area = max(0, int_rect_btm_rght_x - int_rect_top_left_x + 1)*max(0, int_rect_btm_rght_y - int_rect_top_left_y)\n",
    "\n",
    "            #Calculate the IoU.\n",
    "            intersect_over_union = float(int_rect_area / (gt_box_area + anchor_box_area - int_rect_area))\n",
    "\n",
    "            #If the IoU is above or equal to the set threshold, then we want to label it as a positive anchor.\n",
    "            #If it is lower or equal to the set threshold, we want to label it as a negative anchor.\n",
    "            #Normally, there are going to be more negative anchors than the positive ones.\n",
    "            #In an image, there might be more than 1 object. Therefore, an anchor which was labelled as positive for the first object\n",
    "            #can be labelled as negative for the second object. We do not want the positive anchors to be overwritten since positive anchors\n",
    "            #are naturally lower in number. Therefore, if an anchor is already labelled positively, we're not going to label it as negative nor neutral\n",
    "            #on the following objects.\n",
    "            if intersect_over_union >= pos_anchor_thresh:\n",
    "\n",
    "\n",
    "                objectness_label_array[i][0] = 1.0 #positive label is on the left\n",
    "                objectness_label_array[i][1] = 0.0 #overwrite the negative label in case this anchor is labelled negatively for previous object(s).\n",
    "\n",
    "                class_array[i][int(class_label)] = 1.0 #Denote the label of the class in the array.\n",
    "\n",
    "                ##################################################################\n",
    "                #We want to calculate the regression values for positive anchors.#\n",
    "                ##################################################################\n",
    "\n",
    "                #Get the ground-truth box's [x,y,w,h]\n",
    "                gt_box_center_x = ground_truth_boxes[j][0] + ground_truth_boxes[j][2]/2\n",
    "                gt_box_center_y = ground_truth_boxes[j][1] + ground_truth_boxes[j][3]/2\n",
    "                gt_box_width    = ground_truth_boxes[j][2] - ground_truth_boxes[j][0]\n",
    "                gt_box_height   = ground_truth_boxes[j][3] - ground_truth_boxes[j][1]\n",
    "\n",
    "                #Regression loss according to the paper.\n",
    "                delta_x = (gt_box_center_x - anchor[0])/anchor[2]\n",
    "                delta_y = (gt_box_center_y - anchor[1])/anchor[3]\n",
    "                delta_w = math.log(gt_box_width/anchor[2])\n",
    "                delta_h = math.log(gt_box_height/anchor[3])\n",
    "\n",
    "                #Fill in the calculated values in the array.\n",
    "                box_regression_array[i][0] = delta_x\n",
    "                box_regression_array[i][1] = delta_y\n",
    "                box_regression_array[i][2] = delta_w\n",
    "                box_regression_array[i][3] = delta_h\n",
    "\n",
    "            if intersect_over_union <= neg_anchor_thresh:\n",
    "\n",
    "                #Check if the anchor is already labelled positive or not.\n",
    "                if int(objectness_label_array[i][0]) == 0:\n",
    "\n",
    "                    objectness_label_array[i][1] = 1.0\n",
    "\n",
    "            #These are neutral anchors.\n",
    "            if intersect_over_union > neg_anchor_thresh and intersect_over_union < pos_anchor_thresh:\n",
    "\n",
    "                #We do not want to label either the negative or the positive anchors as neutral.\n",
    "                if int(objectness_label_array[i][0]) == 0 and int(objectness_label_array[i][1]) == 0:\n",
    "                    anchor_boolean_array[i][0] = 0.0 #Neutral anchors are to be ignored.\n",
    "\n",
    "\n",
    "    return anchor_boolean_array, objectness_label_array, box_regression_array, class_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anchor_sampling(anchor_booleans, objectness_label, anchor_sampling_amount=anchor_sampling_amount):\n",
    "\n",
    "    '''\n",
    "    Faster R-CNN randomly samples a fixed amount of negative anchors and positive anchors for training. If we use all the neg and pos anchors,\n",
    "    our model will overfit on the negative ones as negative anchors are larger in amount compared to the positive anchors.\n",
    "\n",
    "    Input : anchor booleans and objectness label\n",
    "    Output: Updated anchor booleans. \n",
    "\n",
    "    '''\n",
    "\n",
    "    positive_count = 0\n",
    "    negative_count = 0\n",
    "    \n",
    "\n",
    "    #Iterate through every label.\n",
    "    for i in range(objectness_label.shape[0]):\n",
    "\n",
    "        if int(objectness_label[i][0]) == 1: #If the anchor is positively labelled.\n",
    "\n",
    "            if positive_count > anchor_sampling_amount: #If the positive anchors are more than the threshold amount, set the boolean to 0.\n",
    "\n",
    "                anchor_booleans[i][0] = 0.0\n",
    "\n",
    "            positive_count += 1\n",
    "\n",
    "        if int(objectness_label[i][1]) == 1: #If the anchor is negatively labelled.\n",
    "\n",
    "            if negative_count > anchor_sampling_amount: #If the negative anchors are more than the threshold amount, set the boolean to 0.\n",
    "\n",
    "                anchor_booleans[i][0] = 0.0\n",
    "\n",
    "            negative_count += 1\n",
    "\n",
    "    #Return the updated booleans. REMEMBER! This array was passed by reference. \n",
    "    return anchor_booleans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(first_index, last_index, anchors, anchor_booleans):\n",
    "        '''\n",
    "        This function generates the dataset using the functions defined earlier for the given number of data. If the write status is True,\n",
    "        the output of this function will be written on disk.\n",
    "\n",
    "        Input : starting index and final index of the dataset to be generated.\n",
    "        Output: Anchor booleans, Objectness Label and Regression Label in specified batches.\n",
    "\n",
    "        '''\n",
    "        num_of_anchors = len(anchors)\n",
    "        \n",
    "        batch_anchor_booleans   = []\n",
    "        batch_objectness_array  = []\n",
    "        batch_regression_array  = []\n",
    "        batch_class_label_array = []\n",
    "\n",
    "        for i in range(first_index, last_index):\n",
    "\n",
    "            #Get the true labels and the ground truth boxes [x,y,w,h] for every file.\n",
    "            true_labels, ground_truth_boxes = get_labels_from_xml(xml_file_path=list_annotations[i])\n",
    "\n",
    "            #Get the updated anchor booleans, objectness label and regression label.\n",
    "\n",
    "            anchor_bools, objectness_label_array, box_regression_array, class_array = generate_label(true_labels, ground_truth_boxes, \n",
    "                                                                                                        anchors, anchor_booleans)\n",
    "\n",
    "            #Get the updated anchor bools based on the sampling.\n",
    "            anchor_bools = anchor_sampling(anchor_bools, objectness_label_array)\n",
    "\n",
    "            batch_anchor_booleans.append(anchor_bools)\n",
    "            batch_objectness_array.append(objectness_label_array)\n",
    "            batch_regression_array.append(box_regression_array)\n",
    "            batch_class_label_array.append(class_array)\n",
    "\n",
    "        batch_anchor_booleans   = np.reshape(np.asarray(batch_anchor_booleans), (-1,num_of_anchors))\n",
    "        batch_objectness_array  = np.asarray(batch_objectness_array)\n",
    "        batch_regression_array  = np.asarray(batch_regression_array)\n",
    "        batch_class_label_array = np.asarray(batch_class_label_array)\n",
    "\n",
    "        return (batch_anchor_booleans, batch_objectness_array, batch_regression_array, batch_class_label_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_images(first_index, last_index):\n",
    "    '''\n",
    "    Read the image files, resize, normalize the images and return them in a numpy array.\n",
    "    Input : first and last index.\n",
    "    Output: Numpy array of images.\n",
    "    '''\n",
    "    images_list = []\n",
    "    \n",
    "    for i in range(first_index, last_index):\n",
    "        \n",
    "        im = cv2.imread(list_images[i])\n",
    "        im = cv2.resize(im, (image_height, image_width))/255\n",
    "        \n",
    "        images_list.append(im)\n",
    "    \n",
    "    return np.asarray(images_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchors, an_bools = generate_anchors() #We only need to generate the anchors and the anchor booleans once.\n",
    "num_of_anchors = len(anchors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 6084)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a,b,c,d = generate_dataset(0,1, anchors, an_bools)\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL BUILDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-5\n",
    "epoch = 1000\n",
    "batch_size = 10\n",
    "model_checkpoint = './model_ckpt/model.ckpt'\n",
    "decay_steps = 10000\n",
    "decay_rate = 0.99\n",
    "lambda_value = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_func(t):\n",
    "    \n",
    "    t = tf.abs(t)\n",
    "    \n",
    "    comparison_tensor = tf.ones((num_of_anchors, 4))\n",
    "    smoothed = tf.where(tf.less(t, comparison_tensor), 0.5*tf.pow(t,2), t - 0.5)\n",
    "    \n",
    "    return smoothed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_L1(pred_box, truth_box):\n",
    "    \n",
    "    diff = pred_box - truth_box\n",
    "    \n",
    "    smoothed = tf.map_fn(smooth_func, diff)\n",
    "    \n",
    "    return smoothed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X       = tf.placeholder(tf.float32, shape=(None, image_height, image_width, image_depth)) \n",
    "Y_obj   = tf.placeholder(tf.float32, shape=(None, num_of_anchors,2))\n",
    "Y_coor  = tf.placeholder(tf.float32, shape=(None, num_of_anchors,4))\n",
    "anch_bool = tf.placeholder(tf.float32, shape=(None, num_of_anchors))\n",
    "\n",
    "conv1 = tf.contrib.layers.conv2d(X, num_outputs=64, kernel_size=3, stride=1, \n",
    "                                 padding='SAME', activation_fn=tf.nn.relu)\n",
    "conv2 = tf.contrib.layers.conv2d(conv1, num_outputs=64, kernel_size=3, stride=1, \n",
    "                                 padding='SAME', activation_fn=tf.nn.relu)\n",
    "conv2_pool = tf.nn.max_pool(conv2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "\n",
    "\n",
    "conv3 = tf.contrib.layers.conv2d(conv2_pool, num_outputs=128, kernel_size=3, stride=1, \n",
    "                                 padding='SAME', activation_fn=tf.nn.relu)\n",
    "conv4 = tf.contrib.layers.conv2d(conv3, num_outputs=128, kernel_size=3, stride=1, \n",
    "                                 padding='SAME', activation_fn=tf.nn.relu)\n",
    "conv4_pool = tf.nn.max_pool(conv4, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "\n",
    "conv5 = tf.contrib.layers.conv2d(conv4_pool, num_outputs=512, kernel_size=3, stride=1, \n",
    "                                 padding='SAME', activation_fn=tf.nn.relu)\n",
    "conv6 = tf.contrib.layers.conv2d(conv5, num_outputs=256, kernel_size=3, stride=1, \n",
    "                                 padding='SAME', activation_fn=tf.nn.relu)\n",
    "conv7 = tf.contrib.layers.conv2d(conv6, num_outputs=256, kernel_size=3, stride=1, \n",
    "                                 padding='SAME', activation_fn=tf.nn.relu)\n",
    "conv7_pool = tf.nn.max_pool(conv7, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-18-bc6a3c59f08f>:19: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rpn_conv = tf.contrib.layers.conv2d(conv7_pool, num_outputs=512, kernel_size=3, stride=1, \n",
    "                                    padding='VALID', activation_fn=tf.nn.relu)\n",
    "\n",
    "obj_conv = tf.contrib.layers.conv2d(rpn_conv, num_outputs=18, kernel_size=1, stride=1, padding='VALID', activation_fn=None)\n",
    "bb_conv = tf.contrib.layers.conv2d(rpn_conv, num_outputs=36, kernel_size=1, stride=1, padding='VALID', activation_fn=None)\n",
    "\n",
    "class_conv_reshape = tf.reshape(obj_conv, (-1, num_of_anchors, 2))\n",
    "anchor_conv_reshape = tf.reshape(bb_conv, (-1, num_of_anchors, 4))\n",
    "\n",
    "logits = tf.nn.softmax(class_conv_reshape)\n",
    "\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "decayed_lr = tf.train.exponential_decay(learning_rate,\n",
    "                                            global_step, decay_steps,\n",
    "                                            decay_rate, staircase=True)\n",
    "\n",
    "\n",
    "\n",
    "loss1 = 1/256*tf.reduce_sum(anch_bool*(tf.nn.softmax_cross_entropy_with_logits(labels=Y_obj, logits=class_conv_reshape)))\n",
    "loss2 = lambda_value*(1/128)*tf.reduce_sum((tf.reshape(Y_obj[:,:,0], (-1,num_of_anchors,1)))*smooth_L1(anchor_conv_reshape, Y_coor))\n",
    "\n",
    "total_loss = loss1 + loss2\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(decayed_lr).minimize(total_loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model_ckpt/model.ckpt\n",
      "Model is not loaded!\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "try:\n",
    "    saver.restore(sess, model_checkpoint)\n",
    "    print(\"Model has been loaded!\")\n",
    "    \n",
    "except:\n",
    "    \n",
    "    print(\"Model is not loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 7/1713 [00:16<1:06:54,  2.35s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-a7f89762ce6d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m                                                                   \u001b[0mY_obj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_objectness_array\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                                                                   \u001b[0mY_coor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_regression_array\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m                                                                   anch_bool: batch_anchor_booleans})\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nasa/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nasa/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nasa/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nasa/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nasa/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nasa/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#TRAINING \n",
    "\n",
    "for epoch_idx in range(epoch): #Each epoch.\n",
    "    \n",
    "    #Loop through the whole dataset in batches.\n",
    "    for start_idx in tqdm(range(0, total_images, batch_size)):\n",
    "        \n",
    "        end_idx = start_idx + batch_size\n",
    "        \n",
    "        if end_idx >= total_images : end_idx = total_images - 1 #In case the end index exceeded the dataset.\n",
    "            \n",
    "        images = read_images(start_idx, end_idx) #Read images.\n",
    "        \n",
    "        #Get the labels needed.\n",
    "        batch_anchor_booleans, batch_objectness_array, batch_regression_array, _ = \\\n",
    "                                                generate_dataset(start_idx,end_idx, anchors, an_bools)\n",
    "        #Optimize the model.\n",
    "        _, theloss = sess.run([optimizer, total_loss], feed_dict={X: images,\n",
    "                                                                  Y_obj:batch_objectness_array,\n",
    "                                                                  Y_coor: batch_regression_array,\n",
    "                                                                  anch_bool: batch_anchor_booleans})\n",
    "    #Save the model periodically.\n",
    "    if epoch%10 == 0:\n",
    "        saver.save(sess, model_checkpoint)\n",
    "    \n",
    "    print(\"Epoch : %d, Loss : %g\"%(epoch_idx, theloss))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nasa",
   "language": "python",
   "name": "nasa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
